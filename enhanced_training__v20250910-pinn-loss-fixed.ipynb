{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced CFD Surrogate Model Training with PINN Loss Integration\n",
    "\n",
    "**Î≥ÄÍ≤Ω Ïù¥Ïú† / Ï∞®Ïù¥ ÏöîÏïΩ:**\n",
    "- Î¨ºÎ¶¨ Í∏∞Î∞ò PINN ÏÜêÏã§ Ìï®Ïàò Ï∂îÍ∞Ä (ÏïïÎ†• Íµ¨Î∞∞ÏôÄ Î≤Ω Ï†ÑÎã®ÏùëÎ†• Í¥ÄÍ≥Ñ Í∞ïÌôî)\n",
    "- Ïú†Îèô Î∂ÑÎ¶¨/Î∂ÄÏ∞© ÌòÑÏÉÅÏùÑ Í≥†Î†§Ìïú physics-informed ÌïôÏäµ\n",
    "- adverse pressure gradient ‚Üí low WSS, favorable pressure gradient ‚Üí high WSS Í¥ÄÍ≥Ñ Í∞ïÌôî\n",
    "- WandB ÌÜµÌï© Î∞è Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî Í∏∞Îä• Ïú†ÏßÄ\n",
    "\n",
    "**ÏõêÎ≥∏ ÌååÏùº:** `enhanced_training__v20250910-wandb-integration.ipynb`\n",
    "\n",
    "This notebook integrates advanced PINN loss functions that enforce physical relationships between pressure gradients and wall shear stress, crucial for accurate CFD predictions in separated/attached flow regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CUDA-Enabled CFD Surrogate Model Training\n",
    "# with PINN Loss Integration and WandB Logging\n",
    "# ============================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from typing import Optional, List\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================\n",
    "# CUDA Configuration\n",
    "# ============================================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üìä GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"üîß CUDA Version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Enable TF32 for better performance on Ampere GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Import Required Modules\n",
    "# ============================================\n",
    "\n",
    "from utils import (\n",
    "    load_graphs_with_progress, \n",
    "    preprocess_graph, \n",
    "    fix_y_graph_shape, \n",
    "    compute_dataset_statistics, \n",
    "    cleanup_memory,\n",
    "    compute_relative_l2_error,\n",
    "    compute_node_wise_relative_error,\n",
    "    compute_graph_level_error,\n",
    "    compute_epoch_relative_error,\n",
    "    print_gpu_memory,\n",
    "    clear_gpu_cache,\n",
    "    setup_wandb_logging,\n",
    "    WandBLogger\n",
    ")\n",
    "from model import (\n",
    "    CFDSurrogateModel, \n",
    "    EnsemblePredictor, \n",
    "    LossBalancer, \n",
    "    DataAugmentation\n",
    ")\n",
    "from loss import(\n",
    "    compute_physics_loss,\n",
    "    compute_smoothness_loss,\n",
    "    compute_pinn_loss,\n",
    "    compute_pressure_gradient_loss,\n",
    "    compute_wall_shear_stress_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Data Loading Configuration\n",
    "# ============================================\n",
    "\n",
    "DATA_DIR = Path('/workspace')\n",
    "BATCH_SIZE = 2  # Adjust based on GPU memory\n",
    "\n",
    "print(\"üìÇ Loading graph data...\")\n",
    "train_graphs, val_graphs = load_graphs_with_progress(DATA_DIR)\n",
    "\n",
    "print(\"üîß Preprocessing graphs and moving to CUDA...\")\n",
    "for graphs, split_name in [(train_graphs, 'train'), (val_graphs, 'val')]:\n",
    "    print(f\"  Processing {split_name} graphs...\")\n",
    "    \n",
    "    for i, graph in enumerate(tqdm(graphs, desc=f\"Preprocessing {split_name}\", leave=False)):\n",
    "        # Add area feature if available\n",
    "        if hasattr(graph, 'area') and graph.area is not None:\n",
    "            if hasattr(graph, 'x') and graph.x is not None:\n",
    "                if graph.area.dim() == 1:\n",
    "                    graph.area = graph.area.unsqueeze(-1)\n",
    "                graph.x = torch.cat([graph.x, graph.area], dim=-1)\n",
    "            else:\n",
    "                if hasattr(graph, 'pos'):\n",
    "                    if graph.area.dim() == 1:\n",
    "                        graph.area = graph.area.unsqueeze(-1)\n",
    "                    graph.x = torch.cat([graph.pos, graph.area], dim=-1)\n",
    "        \n",
    "        graph = preprocess_graph(graph)\n",
    "        graph = fix_y_graph_shape(graph)\n",
    "\n",
    "# Update model config\n",
    "if train_graphs and hasattr(train_graphs[0], 'x'):\n",
    "    actual_in_dim = train_graphs[0].x.shape[1]\n",
    "    print(f\"üìä Updated input dimension to: {actual_in_dim}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_graphs, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_graphs, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Dataset statistics\n",
    "print(\"\\nüìà Dataset Statistics:\")\n",
    "dataset_stats = compute_dataset_statistics(train_graphs + val_graphs, verbose=True)\n",
    "\n",
    "# Memory cleanup\n",
    "cleanup_memory()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PINN-Enhanced Training Function with WandB\n",
    "# ============================================\n",
    "\n",
    "def train_with_pinn_and_wandb(model_config=None, loss_weights=None, lr_schedule=None, \n",
    "                             use_ensemble=True, wandb_config=None, **kwargs):\n",
    "    \"\"\"Complete training with PINN loss integration, CUDA support, and WandB logging\n",
    "    \n",
    "    Args:\n",
    "        model_config (dict): Model configuration parameters\n",
    "        loss_weights (dict): Loss weights including PINN loss components\n",
    "        lr_schedule (dict): Learning rate schedule configuration\n",
    "        use_ensemble (bool): Whether to use ensemble for uncertainty quantification\n",
    "        wandb_config (dict): WandB configuration {'project': 'name', 'experiment': 'name', 'enabled': True}\n",
    "        **kwargs: Additional training parameters\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"CFD Surrogate Model Training with PINN Loss Integration & WandB\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Default configurations with PINN loss components\n",
    "    default_model_config = {\n",
    "        'node_feat_dim': 7,\n",
    "        'hidden_dim': 32,\n",
    "        'output_dim': 4,\n",
    "        'num_mp_layers': 3,\n",
    "        'edge_feat_dim': 8,\n",
    "        'use_simple': True\n",
    "    }\n",
    "    \n",
    "    default_loss_weights = {\n",
    "        'mse': 1.0,\n",
    "        'physics': 0.1,\n",
    "        'smoothness': 0.05,\n",
    "        'pinn': 0.2,  # PINN loss weight\n",
    "        'pressure_gradient': 0.15,  # Pressure gradient consistency\n",
    "        'wall_shear_stress': 0.1   # Wall shear stress physics\n",
    "    }\n",
    "    \n",
    "    default_lr_schedule = {\n",
    "        'type': None,\n",
    "        'step_size': 10,\n",
    "        'gamma': 0.1,\n",
    "        'T_max': None,\n",
    "        'eta_min': 1e-6,\n",
    "        'factor': 0.5,\n",
    "        'patience': 5,\n",
    "        'min_lr': 1e-6,\n",
    "        'warmup_epochs': 0,\n",
    "        'warmup_factor': 0.1\n",
    "    }\n",
    "    \n",
    "    default_training_config = {\n",
    "        'epochs': 5,\n",
    "        'lr': 0.001,\n",
    "        'num_ensemble_models': 2\n",
    "    }\n",
    "    \n",
    "    default_wandb_config = {\n",
    "        'project': 'cfd-surrogate-pinn-training',\n",
    "        'experiment': f'pinn_experiment_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "        'enabled': True,\n",
    "        'tags': ['cfd', 'pinn', 'physics-informed', 'graph-neural-network'],\n",
    "        'notes': 'CFD surrogate model training with PINN loss integration'\n",
    "    }\n",
    "    \n",
    "    # Merge configurations\n",
    "    if model_config is not None:\n",
    "        default_model_config.update(model_config)\n",
    "    model_config = default_model_config\n",
    "    \n",
    "    if loss_weights is not None:\n",
    "        default_loss_weights.update(loss_weights)\n",
    "    loss_weights = default_loss_weights\n",
    "    \n",
    "    if lr_schedule is not None:\n",
    "        default_lr_schedule.update(lr_schedule)\n",
    "    lr_schedule = default_lr_schedule\n",
    "    \n",
    "    if wandb_config is not None:\n",
    "        default_wandb_config.update(wandb_config)\n",
    "    wandb_config = default_wandb_config\n",
    "    \n",
    "    # Merge training parameters\n",
    "    training_config = default_training_config.copy()\n",
    "    training_config.update(kwargs)\n",
    "    \n",
    "    # Set T_max for cosine annealing if not specified\n",
    "    if lr_schedule['type'] == 'cosine' and lr_schedule['T_max'] is None:\n",
    "        lr_schedule['T_max'] = training_config['epochs']\n",
    "    \n",
    "    # Initialize WandB logging\n",
    "    print(\"\\n1. Setting up WandB logging...\")\n",
    "    wandb_logger = setup_wandb_logging(\n",
    "        project_name=wandb_config['project'],\n",
    "        experiment_name=wandb_config['experiment'],\n",
    "        tags=wandb_config.get('tags'),\n",
    "        notes=wandb_config.get('notes'),\n",
    "        enabled=wandb_config['enabled']\n",
    "    )\n",
    "    \n",
    "    # Log hyperparameters to WandB\n",
    "    full_config = {\n",
    "        'model': model_config,\n",
    "        'training': training_config,\n",
    "        'loss_weights': loss_weights,\n",
    "        'lr_schedule': lr_schedule,\n",
    "        'use_ensemble': use_ensemble,\n",
    "        'device': str(device),\n",
    "        'dataset': dataset_stats\n",
    "    }\n",
    "    wandb_logger.log_hyperparameters(full_config)\n",
    "    \n",
    "    # Initialize model on CUDA\n",
    "    print(\"\\n2. Initializing model on CUDA...\")\n",
    "    print(f\"   Model config: {model_config}\")\n",
    "    print(f\"   Using ensemble: {use_ensemble}\")\n",
    "    print(f\"   PINN loss enabled: {loss_weights['pinn'] > 0}\")\n",
    "    \n",
    "    model = CFDSurrogateModel(\n",
    "        node_feat_dim=model_config['node_feat_dim'],\n",
    "        hidden_dim=model_config['hidden_dim'],\n",
    "        output_dim=model_config['output_dim'],\n",
    "        num_mp_layers=model_config['num_mp_layers'],\n",
    "        edge_feat_dim=model_config['edge_feat_dim'],\n",
    "        use_simple=model_config['use_simple']\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"   Model on device: {next(model.parameters()).device}\")\n",
    "    print(f\"   Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # Log model architecture to WandB\n",
    "    wandb_logger.log_model_architecture(model)\n",
    "    \n",
    "    # Initialize ensemble or use single model\n",
    "    ensemble = None\n",
    "    if use_ensemble:\n",
    "        print(\"\\n3. Creating ensemble for uncertainty...\")\n",
    "        print(f\"   Ensemble models: {training_config['num_ensemble_models']}\")\n",
    "        print(f\"   ‚ö†Ô∏è Memory usage: ~{training_config['num_ensemble_models']}x single model\")\n",
    "        \n",
    "        ensemble = EnsemblePredictor(\n",
    "            CFDSurrogateModel,\n",
    "            num_models=training_config['num_ensemble_models'],\n",
    "            node_feat_dim=model_config['node_feat_dim'],\n",
    "            hidden_dim=model_config['hidden_dim'],\n",
    "            output_dim=model_config['output_dim'],\n",
    "            num_mp_layers=model_config['num_mp_layers'],\n",
    "            edge_feat_dim=model_config['edge_feat_dim'],\n",
    "            use_simple=model_config['use_simple']\n",
    "        ).to(device)\n",
    "        \n",
    "        training_model = ensemble\n",
    "        wandb_logger.log_model_architecture(ensemble)\n",
    "    else:\n",
    "        print(\"\\n3. Using single model (no ensemble)\")\n",
    "        print(f\"   üíæ Memory saving: No ensemble overhead\")\n",
    "        print(f\"   ‚ö†Ô∏è No uncertainty quantification available\")\n",
    "        training_model = model\n",
    "    \n",
    "    # Training components\n",
    "    print(f\"\\n4. Setting up PINN-enhanced training with lr={training_config['lr']}\")\n",
    "    print(f\"   Loss weights: {loss_weights}\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(training_model.parameters(), lr=training_config['lr'])\n",
    "    loss_balancer = LossBalancer(num_losses=6)  # Updated for PINN losses\n",
    "    augmentation = DataAugmentation()\n",
    "    \n",
    "    # Learning rate scheduler setup\n",
    "    scheduler = None\n",
    "    warmup_scheduler = None\n",
    "    \n",
    "    if lr_schedule['type'] is not None:\n",
    "        print(f\"   LR Schedule: {lr_schedule['type']} - {lr_schedule}\")\n",
    "        \n",
    "        if lr_schedule['type'] == 'step':\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                optimizer, step_size=lr_schedule['step_size'], gamma=lr_schedule['gamma'])\n",
    "        elif lr_schedule['type'] == 'cosine':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, T_max=lr_schedule['T_max'], eta_min=lr_schedule['eta_min'])\n",
    "        elif lr_schedule['type'] == 'exponential':\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "                optimizer, gamma=lr_schedule['gamma'])\n",
    "        elif lr_schedule['type'] == 'reduce_on_plateau':\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='min', factor=lr_schedule['factor'],\n",
    "                patience=lr_schedule['patience'], min_lr=lr_schedule['min_lr'])\n",
    "        \n",
    "        if lr_schedule['warmup_epochs'] > 0:\n",
    "            warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "                optimizer, start_factor=lr_schedule['warmup_factor'], end_factor=1.0,\n",
    "                total_iters=lr_schedule['warmup_epochs'])\n",
    "            print(f\"   Warmup: {lr_schedule['warmup_epochs']} epochs from {lr_schedule['warmup_factor']} to 1.0\")\n",
    "    \n",
    "    # Mixed precision training\n",
    "    use_amp = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 7\n",
    "    if use_amp:\n",
    "        print(\"\\n5. Using Automatic Mixed Precision (AMP) with PINN losses\")\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    # Training history\n",
    "    train_history = {\n",
    "        'loss': [], 'rel_l2': [], 'rel_l2_std': [], 'per_channel': [], 'lr': [],\n",
    "        'pinn_loss': [], 'pressure_grad_loss': [], 'wall_shear_loss': []\n",
    "    }\n",
    "    val_history = {\n",
    "        'loss': [], 'rel_l2': [], 'rel_l2_std': [], 'per_channel': []\n",
    "    }\n",
    "    \n",
    "    # Training loop with PINN losses and WandB logging\n",
    "    print(f\"\\n6. Starting PINN-enhanced training for {training_config['epochs']} epochs...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(training_config['epochs']):\n",
    "        epoch_start_time = time.time()\n",
    "        epoch_loss = 0\n",
    "        epoch_mse_loss = 0\n",
    "        epoch_physics_loss = 0\n",
    "        epoch_smooth_loss = 0\n",
    "        epoch_pinn_loss = 0\n",
    "        epoch_pressure_grad_loss = 0\n",
    "        epoch_wall_shear_loss = 0\n",
    "        batch_rel_l2_errors = []\n",
    "        training_model.train()\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        train_history['lr'].append(current_lr)\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            try:\n",
    "                batch = batch.to(device)\n",
    "                \n",
    "                # Data augmentation\n",
    "                if epoch % 2 == 0:\n",
    "                    batch = augmentation.add_noise(batch, 0.01)\n",
    "                \n",
    "                # Forward pass\n",
    "                if use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if use_ensemble:\n",
    "                            pred_mean, pred_std = training_model(batch)\n",
    "                        else:\n",
    "                            pred_mean = training_model(batch)\n",
    "                            pred_std = None\n",
    "                        \n",
    "                        # Standard losses\n",
    "                        mse_loss = F.mse_loss(pred_mean, batch.y)\n",
    "                        physics_loss = compute_physics_loss(pred_mean, batch)\n",
    "                        smooth_loss = compute_smoothness_loss(pred_mean, batch.edge_index)\n",
    "                        \n",
    "                        # PINN losses\n",
    "                        pinn_loss = compute_pinn_loss(pred_mean, batch)\n",
    "                        pressure_grad_loss = compute_pressure_gradient_loss(pred_mean, batch)\n",
    "                        wall_shear_loss = compute_wall_shear_stress_loss(pred_mean, batch)\n",
    "                else:\n",
    "                    if use_ensemble:\n",
    "                        pred_mean, pred_std = training_model(batch)\n",
    "                    else:\n",
    "                        pred_mean = training_model(batch)\n",
    "                        pred_std = None\n",
    "                    \n",
    "                    # Standard losses\n",
    "                    mse_loss = F.mse_loss(pred_mean, batch.y)\n",
    "                    physics_loss = compute_physics_loss(pred_mean, batch)\n",
    "                    smooth_loss = compute_smoothness_loss(pred_mean, batch.edge_index)\n",
    "                    \n",
    "                    # PINN losses\n",
    "                    pinn_loss = compute_pinn_loss(pred_mean, batch)\n",
    "                    pressure_grad_loss = compute_pressure_gradient_loss(pred_mean, batch)\n",
    "                    wall_shear_loss = compute_wall_shear_stress_loss(pred_mean, batch)\n",
    "                \n",
    "                # Compute relative L2 error for monitoring\n",
    "                with torch.no_grad():\n",
    "                    rel_l2, per_channel = compute_relative_l2_error(pred_mean, batch.y)\n",
    "                    batch_rel_l2_errors.append(rel_l2.item())\n",
    "                \n",
    "                # Apply loss weights including PINN components\n",
    "                total_loss = (\n",
    "                    loss_weights['mse'] * mse_loss + \n",
    "                    loss_weights['physics'] * physics_loss + \n",
    "                    loss_weights['smoothness'] * smooth_loss +\n",
    "                    loss_weights['pinn'] * pinn_loss +\n",
    "                    loss_weights['pressure_gradient'] * pressure_grad_loss +\n",
    "                    loss_weights['wall_shear_stress'] * wall_shear_loss\n",
    "                )\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if use_amp:\n",
    "                    scaler.scale(total_loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(training_model.parameters(), max_norm=1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    total_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(training_model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # Accumulate losses for logging\n",
    "                epoch_loss += total_loss.item()\n",
    "                epoch_mse_loss += mse_loss.item()\n",
    "                epoch_physics_loss += physics_loss.item()\n",
    "                epoch_smooth_loss += smooth_loss.item()\n",
    "                epoch_pinn_loss += pinn_loss.item()\n",
    "                epoch_pressure_grad_loss += pressure_grad_loss.item()\n",
    "                epoch_wall_shear_loss += wall_shear_loss.item()\n",
    "                \n",
    "                # Print batch progress\n",
    "                if batch_idx % 5 == 0:\n",
    "                    gpu_mem = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "                    uncertainty_info = f\"Std = {pred_std.mean().item():.4f}, \" if pred_std is not None else \"\"\n",
    "                    print(f\"   Batch {batch_idx}/{len(train_loader)}: \"\n",
    "                          f\"Loss = {total_loss.item():.4f}, \"\n",
    "                          f\"PINN = {pinn_loss.item():.4f}, \"\n",
    "                          f\"PG = {pressure_grad_loss.item():.4f}, \"\n",
    "                          f\"WS = {wall_shear_loss.item():.4f}, \"\n",
    "                          f\"Rel L2 = {rel_l2.item():.4f}, \"\n",
    "                          f\"{uncertainty_info}\"\n",
    "                          f\"LR = {current_lr:.2e}, \"\n",
    "                          f\"GPU = {gpu_mem:.2f}GB\")\n",
    "                \n",
    "                # Clear cache periodically\n",
    "                if torch.cuda.is_available() and batch_idx % 10 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    print(f\"   ‚ö†Ô∏è OOM in batch {batch_idx}, clearing cache...\")\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"   Error in batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # Compute epoch metrics\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        avg_mse_loss = epoch_mse_loss / len(train_loader)\n",
    "        avg_physics_loss = epoch_physics_loss / len(train_loader)\n",
    "        avg_smooth_loss = epoch_smooth_loss / len(train_loader)\n",
    "        avg_pinn_loss = epoch_pinn_loss / len(train_loader)\n",
    "        avg_pressure_grad_loss = epoch_pressure_grad_loss / len(train_loader)\n",
    "        avg_wall_shear_loss = epoch_wall_shear_loss / len(train_loader)\n",
    "        \n",
    "        # Store PINN loss history\n",
    "        train_history['pinn_loss'].append(avg_pinn_loss)\n",
    "        train_history['pressure_grad_loss'].append(avg_pressure_grad_loss)\n",
    "        train_history['wall_shear_loss'].append(avg_wall_shear_loss)\n",
    "        \n",
    "        # Log PINN loss components to WandB\n",
    "        wandb_logger.log_loss_components(\n",
    "            epoch=epoch + 1,\n",
    "            mse_loss=avg_mse_loss,\n",
    "            physics_loss=avg_physics_loss,\n",
    "            smoothness_loss=avg_smooth_loss,\n",
    "            pinn_loss=avg_pinn_loss,\n",
    "            pressure_gradient_loss=avg_pressure_grad_loss,\n",
    "            wall_shear_stress_loss=avg_wall_shear_loss,\n",
    "            loss_weights=loss_weights\n",
    "        )\n",
    "        \n",
    "        # Compute training set relative L2 error\n",
    "        print(f\"\\n   üìä Computing epoch {epoch+1} training metrics...\")\n",
    "        train_rel_l2, train_std_l2, train_per_channel, _ = compute_epoch_relative_error(\n",
    "            training_model, train_loader, device, use_ensemble=use_ensemble\n",
    "        )\n",
    "        \n",
    "        # Store training history\n",
    "        train_history['loss'].append(avg_loss)\n",
    "        train_history['rel_l2'].append(train_rel_l2)\n",
    "        train_history['rel_l2_std'].append(train_std_l2)\n",
    "        train_history['per_channel'].append(train_per_channel.cpu().numpy())\n",
    "        \n",
    "        # Validation metrics\n",
    "        print(f\"\\n   üìä Computing epoch {epoch+1} validation metrics...\")\n",
    "        val_rel_l2, val_std_l2, val_per_channel, _ = compute_epoch_relative_error(\n",
    "            training_model, val_loader, device, use_ensemble=use_ensemble\n",
    "        )\n",
    "        \n",
    "        val_history['rel_l2'].append(val_rel_l2)\n",
    "        val_history['rel_l2_std'].append(val_std_l2)\n",
    "        val_history['per_channel'].append(val_per_channel.cpu().numpy())\n",
    "        \n",
    "        # Collect uncertainty statistics for ensemble\n",
    "        uncertainty_stats = None\n",
    "        if use_ensemble:\n",
    "            # Sample a batch to get uncertainty statistics\n",
    "            training_model.eval()\n",
    "            with torch.no_grad():\n",
    "                for sample_batch in val_loader:\n",
    "                    sample_batch = sample_batch.to(device)\n",
    "                    _, sample_std = training_model(sample_batch)\n",
    "                    uncertainty_stats = {\n",
    "                        'mean_uncertainty': sample_std.mean().item(),\n",
    "                        'max_uncertainty': sample_std.max().item(),\n",
    "                        'std_uncertainty': sample_std.std().item()\n",
    "                    }\n",
    "                    break\n",
    "        \n",
    "        # Get current GPU memory\n",
    "        current_gpu_memory = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else None\n",
    "        \n",
    "        # Log comprehensive training metrics to WandB\n",
    "        wandb_logger.log_training_metrics(\n",
    "            epoch=epoch + 1,\n",
    "            train_loss=avg_loss,\n",
    "            train_rel_l2=train_rel_l2,\n",
    "            val_rel_l2=val_rel_l2,\n",
    "            learning_rate=current_lr,\n",
    "            gpu_memory=current_gpu_memory,\n",
    "            train_per_channel=train_per_channel.cpu().numpy(),\n",
    "            val_per_channel=val_per_channel.cpu().numpy(),\n",
    "            uncertainty_stats=uncertainty_stats\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        if scheduler is not None:\n",
    "            if lr_schedule['type'] == 'reduce_on_plateau':\n",
    "                scheduler.step(val_rel_l2)\n",
    "            else:\n",
    "                if warmup_scheduler is not None and epoch < lr_schedule['warmup_epochs']:\n",
    "                    warmup_scheduler.step()\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "        \n",
    "        # Update current learning rate for next epoch\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Log epoch time\n",
    "        wandb_logger.log({\"system/epoch_time_seconds\": epoch_time}, step=epoch + 1, commit=False)\n",
    "        \n",
    "        print(f\"\\n   ===== EPOCH {epoch+1}/{training_config['epochs']} SUMMARY (PINN) =====\")\n",
    "        print(f\"   Training Loss: {avg_loss:.4f} (MSE: {avg_mse_loss:.4f}, Physics: {avg_physics_loss:.4f})\")\n",
    "        print(f\"   PINN Losses: PINN: {avg_pinn_loss:.4f}, PressGrad: {avg_pressure_grad_loss:.4f}, WallShear: {avg_wall_shear_loss:.4f}\")\n",
    "        print(f\"   Training Rel L2: {train_rel_l2:.4f} ¬± {train_std_l2:.4f}\")\n",
    "        print(f\"   Validation Rel L2: {val_rel_l2:.4f} ¬± {val_std_l2:.4f}\")\n",
    "        print(f\"   Learning Rate: {current_lr:.2e} ‚Üí {new_lr:.2e}\")\n",
    "        print(f\"   Epoch Time: {epoch_time:.1f}s\")\n",
    "        if use_ensemble and uncertainty_stats:\n",
    "            print(f\"   Uncertainty: {uncertainty_stats['mean_uncertainty']:.4f} ¬± {uncertainty_stats['std_uncertainty']:.4f}\")\n",
    "        else:\n",
    "            print(f\"   Mode: Single model with PINN (memory optimized)\")\n",
    "        print(\"   \" + \"=\"*60)\n",
    "    \n",
    "    total_training_time = time.time() - training_start_time\n",
    "    wandb_logger.log({\"system/total_training_time_seconds\": total_training_time})\n",
    "    \n",
    "    print(\"\\n‚úî PINN-enhanced training completed successfully!\")\n",
    "    print(f\"   Total training time: {total_training_time/60:.2f} minutes\")\n",
    "    \n",
    "    # Print training summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìà PINN-ENHANCED TRAINING SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìã Configuration:\")\n",
    "    print(f\"   Model: {model_config}\")\n",
    "    print(f\"   Training: epochs={training_config['epochs']}, lr={training_config['lr']}\")\n",
    "    print(f\"   Loss weights: {loss_weights}\")\n",
    "    print(f\"   Ensemble: {use_ensemble} ({'enabled' if use_ensemble else 'disabled for memory optimization'})\")\n",
    "    print(f\"   WandB: {'enabled' if wandb_config['enabled'] else 'disabled'}\")\n",
    "    \n",
    "    print(\"\\nüìä Final Results:\")\n",
    "    best_train_epoch = np.argmin(train_history['rel_l2']) + 1\n",
    "    best_val_epoch = np.argmin(val_history['rel_l2']) + 1\n",
    "    print(f\"   Best Training Rel L2: {min(train_history['rel_l2']):.4f} (Epoch {best_train_epoch})\")\n",
    "    print(f\"   Best Validation Rel L2: {min(val_history['rel_l2']):.4f} (Epoch {best_val_epoch})\")\n",
    "    \n",
    "    print(\"\\nüß† PINN Loss Evolution:\")\n",
    "    print(f\"   Final PINN Loss: {train_history['pinn_loss'][-1]:.4f}\")\n",
    "    print(f\"   Final Pressure Gradient Loss: {train_history['pressure_grad_loss'][-1]:.4f}\")\n",
    "    print(f\"   Final Wall Shear Stress Loss: {train_history['wall_shear_loss'][-1]:.4f}\")\n",
    "    \n",
    "    # Log final summary to WandB\n",
    "    wandb_logger.log({\n",
    "        \"summary/best_train_rel_l2\": min(train_history['rel_l2']),\n",
    "        \"summary/best_val_rel_l2\": min(val_history['rel_l2']),\n",
    "        \"summary/final_lr\": train_history['lr'][-1],\n",
    "        \"summary/total_epochs\": training_config['epochs'],\n",
    "        \"summary/total_training_time_minutes\": total_training_time / 60,\n",
    "        \"summary/final_pinn_loss\": train_history['pinn_loss'][-1],\n",
    "        \"summary/final_pressure_grad_loss\": train_history['pressure_grad_loss'][-1],\n",
    "        \"summary/final_wall_shear_loss\": train_history['wall_shear_loss'][-1]\n",
    "    })\n",
    "    \n",
    "    # Finish WandB run\n",
    "    wandb_logger.finish()\n",
    "    \n",
    "    return model, ensemble if use_ensemble else None, train_history, val_history, wandb_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Enhanced Inference Function with PINN Analysis\n",
    "# ============================================\n",
    "\n",
    "def inference_with_pinn_analysis(model=None, ensemble=None, use_ensemble=None, \n",
    "                               wandb_logger=None):\n",
    "    \"\"\"Inference with PINN-specific analysis and WandB logging\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PINN-Enhanced Inference with Physics Analysis\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Auto-detect which model to use\n",
    "    if use_ensemble is None:\n",
    "        use_ensemble = ensemble is not None\n",
    "    \n",
    "    inference_model = ensemble if use_ensemble else model\n",
    "    \n",
    "    if inference_model is None:\n",
    "        print(\"‚ùå No model provided for inference!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üîß Using {'ensemble' if use_ensemble else 'single'} model for PINN inference\")\n",
    "    \n",
    "    inference_model.eval()\n",
    "    \n",
    "    print(\"\\n1. Running PINN-enhanced inference analysis...\")\n",
    "    \n",
    "    inference_stats = {\n",
    "        'total_time_ms': 0,\n",
    "        'avg_rel_l2': 0,\n",
    "        'uncertainty_stats': {},\n",
    "        'error_stats': {},\n",
    "        'physics_consistency': {}\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        total_rel_l2 = 0\n",
    "        physics_violations = []\n",
    "        \n",
    "        for i, batch in enumerate(val_loader):\n",
    "            if i >= 3:  # Process first 3 batches for comprehensive analysis\n",
    "                break\n",
    "            \n",
    "            batch = batch.to(device)\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Time the inference\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "                start = torch.cuda.Event(enable_timing=True)\n",
    "                end = torch.cuda.Event(enable_timing=True)\n",
    "                \n",
    "                start.record()\n",
    "                if use_ensemble:\n",
    "                    pred_mean, pred_std = inference_model(batch)\n",
    "                else:\n",
    "                    pred_mean = inference_model(batch)\n",
    "                    pred_std = None\n",
    "                end.record()\n",
    "                \n",
    "                torch.cuda.synchronize()\n",
    "                batch_time = start.elapsed_time(end)\n",
    "                inference_stats['total_time_ms'] += batch_time\n",
    "                print(f\"   Batch {i+1} inference time: {batch_time:.2f} ms\")\n",
    "            else:\n",
    "                start_time = time.time()\n",
    "                if use_ensemble:\n",
    "                    pred_mean, pred_std = inference_model(batch)\n",
    "                else:\n",
    "                    pred_mean = inference_model(batch)\n",
    "                    pred_std = None\n",
    "                batch_time = (time.time() - start_time) * 1000\n",
    "                inference_stats['total_time_ms'] += batch_time\n",
    "            \n",
    "            print(f\"   üì¶ Batch {i+1} size: {batch.x.shape[0]} nodes\")\n",
    "            \n",
    "            # Compute relative L2 error\n",
    "            rel_l2, per_channel = compute_relative_l2_error(pred_mean, batch.y)\n",
    "            total_rel_l2 += rel_l2.item()\n",
    "            print(f\"   üìä Batch {i+1} Relative L2 Error: {rel_l2.item():.4f}\")\n",
    "            print(f\"   Per-channel: {per_channel.numpy()}\")\n",
    "            \n",
    "            # PINN-specific physics consistency checks\n",
    "            print(f\"\\n   üß† Batch {i+1} Physics Consistency Analysis:\")\n",
    "            \n",
    "            # Check pressure gradient consistency\n",
    "            try:\n",
    "                pg_loss = compute_pressure_gradient_loss(pred_mean, batch)\n",
    "                print(f\"   - Pressure gradient consistency: {pg_loss.item():.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   - Pressure gradient check failed: {e}\")\n",
    "                pg_loss = torch.tensor(float('nan'))\n",
    "            \n",
    "            # Check wall shear stress physics\n",
    "            try:\n",
    "                wss_loss = compute_wall_shear_stress_loss(pred_mean, batch)\n",
    "                print(f\"   - Wall shear stress physics: {wss_loss.item():.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   - Wall shear stress check failed: {e}\")\n",
    "                wss_loss = torch.tensor(float('nan'))\n",
    "            \n",
    "            # Overall PINN loss\n",
    "            try:\n",
    "                pinn_loss = compute_pinn_loss(pred_mean, batch)\n",
    "                print(f\"   - Overall PINN physics violation: {pinn_loss.item():.4f}\")\n",
    "                physics_violations.append(pinn_loss.item())\n",
    "            except Exception as e:\n",
    "                print(f\"   - PINN loss check failed: {e}\")\n",
    "                pinn_loss = torch.tensor(float('nan'))\n",
    "            \n",
    "            # Uncertainty analysis (only for ensemble)\n",
    "            if use_ensemble and pred_std is not None:\n",
    "                mean_uncertainty = pred_std.mean().item()\n",
    "                max_uncertainty = pred_std.max().item()\n",
    "                std_uncertainty = pred_std.std().item()\n",
    "                \n",
    "                print(f\"\\n   üéØ Batch {i+1} Uncertainty Statistics:\")\n",
    "                print(f\"   - Mean uncertainty: {mean_uncertainty:.4f}\")\n",
    "                print(f\"   - Max uncertainty: {max_uncertainty:.4f}\")\n",
    "                print(f\"   - Std uncertainty: {std_uncertainty:.4f}\")\n",
    "                \n",
    "                # Store uncertainty stats for first batch\n",
    "                if i == 0:\n",
    "                    inference_stats['uncertainty_stats'] = {\n",
    "                        'mean': mean_uncertainty,\n",
    "                        'max': max_uncertainty,\n",
    "                        'std': std_uncertainty\n",
    "                    }\n",
    "            \n",
    "            # Detailed error analysis for first batch\n",
    "            if i == 0:\n",
    "                node_errors, mean_err, max_err, percentiles = compute_node_wise_relative_error(\n",
    "                    pred_mean, batch.y\n",
    "                )\n",
    "                \n",
    "                print(f\"\\n   üìà Detailed Node-wise Error Statistics:\")\n",
    "                print(f\"   - Mean: {mean_err.item():.4f}\")\n",
    "                print(f\"   - Max: {max_err.item():.4f}\")\n",
    "                print(f\"   - 25th percentile: {percentiles[0].item():.4f}\")\n",
    "                print(f\"   - Median: {percentiles[1].item():.4f}\")\n",
    "                print(f\"   - 75th percentile: {percentiles[2].item():.4f}\")\n",
    "                print(f\"   - 95th percentile: {percentiles[3].item():.4f}\")\n",
    "                \n",
    "                inference_stats['error_stats'] = {\n",
    "                    'mean_error': mean_err.item(),\n",
    "                    'max_error': max_err.item(),\n",
    "                    'median_error': percentiles[1].item(),\n",
    "                    'p95_error': percentiles[3].item()\n",
    "                }\n",
    "                \n",
    "                # Store physics consistency stats\n",
    "                inference_stats['physics_consistency'] = {\n",
    "                    'pressure_gradient_loss': pg_loss.item() if not torch.isnan(pg_loss) else None,\n",
    "                    'wall_shear_stress_loss': wss_loss.item() if not torch.isnan(wss_loss) else None,\n",
    "                    'pinn_loss': pinn_loss.item() if not torch.isnan(pinn_loss) else None\n",
    "                }\n",
    "                \n",
    "                # Find worst predictions\n",
    "                worst_indices = torch.topk(node_errors, k=min(5, len(node_errors)))[1]\n",
    "                print(f\"   üîç Worst prediction nodes: {worst_indices.cpu().numpy()}\")\n",
    "        \n",
    "        # Calculate averages\n",
    "        inference_stats['avg_time_ms'] = inference_stats['total_time_ms'] / batch_count\n",
    "        inference_stats['avg_rel_l2'] = total_rel_l2 / batch_count\n",
    "        inference_stats['avg_physics_violation'] = np.mean(physics_violations) if physics_violations else None\n",
    "        \n",
    "        print(f\"\\nüìä Overall PINN-Enhanced Inference Statistics:\")\n",
    "        print(f\"   Average inference time: {inference_stats['avg_time_ms']:.2f} ms/batch\")\n",
    "        print(f\"   Average relative L2 error: {inference_stats['avg_rel_l2']:.4f}\")\n",
    "        if inference_stats['avg_physics_violation'] is not None:\n",
    "            print(f\"   Average physics violation: {inference_stats['avg_physics_violation']:.4f}\")\n",
    "        \n",
    "        # Log to WandB if logger provided\n",
    "        if wandb_logger is not None:\n",
    "            wandb_logger.log_inference_results(\n",
    "                inference_time=inference_stats['avg_time_ms'],\n",
    "                uncertainty_stats=inference_stats['uncertainty_stats'],\n",
    "                error_stats=inference_stats['error_stats'],\n",
    "                physics_stats=inference_stats['physics_consistency']\n",
    "            )\n",
    "    \n",
    "    print(\"\\n‚úî PINN-enhanced inference and physics analysis completed!\")\n",
    "    return inference_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PINN Training Examples with WandB Integration\n",
    "# ============================================\n",
    "\n",
    "# Example 1: Memory-efficient PINN training with WandB\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE 1: Memory-Efficient PINN Training with WandB Logging\")\n",
    "print(\"üíæ PINN physics + memory optimization + comprehensive tracking\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pinn_wandb_config_1 = {\n",
    "    'project': 'cfd-surrogate-pinn-experiments',\n",
    "    'experiment': 'pinn_single_model_baseline',\n",
    "    'enabled': True,  # Set to False to disable WandB\n",
    "    'tags': ['pinn', 'memory-optimized', 'single-model', 'physics-informed'],\n",
    "    'notes': 'Memory-efficient PINN training with physics-informed losses for VRAM-limited setups'\n",
    "}\n",
    "\n",
    "pinn_loss_weights_1 = {\n",
    "    'mse': 1.0,\n",
    "    'physics': 0.1,\n",
    "    'smoothness': 0.05,\n",
    "    'pinn': 0.3,  # Strong PINN enforcement\n",
    "    'pressure_gradient': 0.2,\n",
    "    'wall_shear_stress': 0.15\n",
    "}\n",
    "\n",
    "model1, ensemble1, train_hist1, val_hist1, logger1 = train_with_pinn_and_wandb(\n",
    "    loss_weights=pinn_loss_weights_1,\n",
    "    use_ensemble=False,\n",
    "    wandb_config=pinn_wandb_config_1,\n",
    "    epochs=8,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "# Run PINN-enhanced inference\n",
    "inference_stats1 = inference_with_pinn_analysis(model=model1, wandb_logger=logger1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: PINN Ensemble with advanced physics constraints\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE 2: PINN Ensemble + Advanced Physics Constraints\")\n",
    "print(\"üß† Enhanced physics enforcement + uncertainty quantification\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "advanced_pinn_loss_weights = {\n",
    "    'mse': 1.2,\n",
    "    'physics': 0.15,\n",
    "    'smoothness': 0.08,\n",
    "    'pinn': 0.4,  # Very strong PINN enforcement\n",
    "    'pressure_gradient': 0.25,  # Enhanced pressure gradient consistency\n",
    "    'wall_shear_stress': 0.2   # Strong wall shear stress physics\n",
    "}\n",
    "\n",
    "cosine_lr_schedule = {\n",
    "    'type': 'cosine',\n",
    "    'T_max': 12,\n",
    "    'eta_min': 1e-6,\n",
    "    'warmup_epochs': 2,\n",
    "    'warmup_factor': 0.1\n",
    "}\n",
    "\n",
    "pinn_wandb_config_2 = {\n",
    "    'project': 'cfd-surrogate-pinn-experiments',\n",
    "    'experiment': 'pinn_ensemble_advanced_physics',\n",
    "    'enabled': True,\n",
    "    'tags': ['pinn', 'ensemble', 'advanced-physics', 'uncertainty', 'cosine-lr'],\n",
    "    'notes': 'PINN ensemble with advanced physics constraints and uncertainty quantification'\n",
    "}\n",
    "\n",
    "model2, ensemble2, train_hist2, val_hist2, logger2 = train_with_pinn_and_wandb(\n",
    "    loss_weights=advanced_pinn_loss_weights,\n",
    "    lr_schedule=cosine_lr_schedule,\n",
    "    use_ensemble=True,\n",
    "    num_ensemble_models=2,  # Small ensemble for memory efficiency\n",
    "    wandb_config=pinn_wandb_config_2,\n",
    "    epochs=12,\n",
    "    lr=0.002\n",
    ")\n",
    "\n",
    "# Run PINN-enhanced inference with uncertainty analysis\n",
    "inference_stats2 = inference_with_pinn_analysis(\n",
    "    model=model2, ensemble=ensemble2, wandb_logger=logger2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Fine-tuned PINN with custom model architecture\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE 3: Fine-Tuned PINN with Custom Architecture\")\n",
    "print(\"üî¨ Larger model + balanced PINN constraints + plateau LR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pinn_model_config = {\n",
    "    'hidden_dim': 64,  # Larger model for better physics learning\n",
    "    'num_mp_layers': 4\n",
    "}\n",
    "\n",
    "balanced_pinn_loss_weights = {\n",
    "    'mse': 1.0,\n",
    "    'physics': 0.12,\n",
    "    'smoothness': 0.08,\n",
    "    'pinn': 0.25,  # Balanced PINN enforcement\n",
    "    'pressure_gradient': 0.18,  # Moderate pressure gradient focus\n",
    "    'wall_shear_stress': 0.15   # Moderate wall shear stress focus\n",
    "}\n",
    "\n",
    "plateau_lr_schedule = {\n",
    "    'type': 'reduce_on_plateau',\n",
    "    'factor': 0.5,\n",
    "    'patience': 3,\n",
    "    'min_lr': 1e-6\n",
    "}\n",
    "\n",
    "pinn_wandb_config_3 = {\n",
    "    'project': 'cfd-surrogate-pinn-experiments',\n",
    "    'experiment': 'pinn_custom_architecture_balanced',\n",
    "    'enabled': True,\n",
    "    'tags': ['pinn', 'custom-architecture', 'balanced-physics', 'plateau-lr'],\n",
    "    'notes': 'Custom architecture PINN with balanced physics constraints and adaptive LR'\n",
    "}\n",
    "\n",
    "# Choose ensemble based on available memory (adjust as needed)\n",
    "use_ensemble_pinn = False  # Set to True if you have sufficient VRAM\n",
    "\n",
    "model3, ensemble3, train_hist3, val_hist3, logger3 = train_with_pinn_and_wandb(\n",
    "    model_config=pinn_model_config,\n",
    "    loss_weights=balanced_pinn_loss_weights,\n",
    "    lr_schedule=plateau_lr_schedule,\n",
    "    use_ensemble=use_ensemble_pinn,\n",
    "    num_ensemble_models=2 if use_ensemble_pinn else 1,\n",
    "    wandb_config=pinn_wandb_config_3,\n",
    "    epochs=15,\n",
    "    lr=0.0015\n",
    ")\n",
    "\n",
    "# Run PINN-enhanced inference\n",
    "inference_stats3 = inference_with_pinn_analysis(\n",
    "    model=model3, ensemble=ensemble3, wandb_logger=logger3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PINN Experiment Comparison and Analysis\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä PINN EXPERIMENT COMPARISON & PHYSICS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pinn_experiments = [\n",
    "    (\"Memory-Efficient PINN\", train_hist1, val_hist1, False, inference_stats1),\n",
    "    (\"PINN Ensemble Advanced\", train_hist2, val_hist2, True, inference_stats2),\n",
    "    (\"Custom Architecture PINN\", train_hist3, val_hist3, use_ensemble_pinn, inference_stats3)\n",
    "]\n",
    "\n",
    "print(\"\\nüìã PINN Performance vs Physics Consistency Comparison:\")\n",
    "print(\"Experiment                  | Best Val L2 | Physics Viol. | Memory Mode   | Uncertainty\")\n",
    "print(\"----------------------------|-------------|---------------|---------------|-------------\")\n",
    "\n",
    "best_val_errors = []\n",
    "physics_violations = []\n",
    "\n",
    "for name, train_h, val_h, has_ensemble, inf_stats in pinn_experiments:\n",
    "    best_val_l2 = min(val_h['rel_l2'])\n",
    "    best_val_errors.append(best_val_l2)\n",
    "    \n",
    "    physics_viol = inf_stats.get('avg_physics_violation', 'N/A')\n",
    "    if physics_viol != 'N/A' and physics_viol is not None:\n",
    "        physics_violations.append(physics_viol)\n",
    "        physics_str = f\"{physics_viol:.4f}\"\n",
    "    else:\n",
    "        physics_str = \"N/A\"\n",
    "    \n",
    "    memory_mode = \"High (Ensemble)\" if has_ensemble else \"Low (Single)\"\n",
    "    uncertainty = \"Yes\" if has_ensemble else \"No\"\n",
    "    \n",
    "    marker = \"üèÜ\" if best_val_l2 == min(best_val_errors) else \"  \"\n",
    "    print(f\"{marker} {name:<25} | {best_val_l2:.4f}      | {physics_str:<13} | {memory_mode:<13} | {uncertainty:<11}\")\n",
    "\n",
    "print(f\"\\nüß† PINN-Specific Insights:\")\n",
    "print(f\"   ‚Ä¢ Physics-Informed Losses: Enforces CFD physics in neural network training\")\n",
    "print(f\"   ‚Ä¢ Pressure Gradient Consistency: Ensures realistic flow field predictions\")\n",
    "print(f\"   ‚Ä¢ Wall Shear Stress Physics: Maintains boundary layer accuracy\")\n",
    "print(f\"   ‚Ä¢ Memory vs Physics: Single models still maintain physics constraints\")\n",
    "print(f\"   ‚Ä¢ WandB Integration: All physics losses tracked and visualizable\")\n",
    "\n",
    "if physics_violations:\n",
    "    best_physics_idx = np.argmin(physics_violations)\n",
    "    best_physics_name = pinn_experiments[best_physics_idx][0]\n",
    "    print(f\"   ‚Ä¢ Best Physics Consistency: {best_physics_name} (violation: {min(physics_violations):.4f})\")\n",
    "\n",
    "print(f\"\\nüìà PINN Loss Evolution Analysis:\")\n",
    "for i, (name, train_h, _, _, _) in enumerate(pinn_experiments):\n",
    "    if 'pinn_loss' in train_h and train_h['pinn_loss']:\n",
    "        initial_pinn = train_h['pinn_loss'][0]\n",
    "        final_pinn = train_h['pinn_loss'][-1]\n",
    "        improvement = ((initial_pinn - final_pinn) / initial_pinn) * 100\n",
    "        print(f\"   ‚Ä¢ {name}: PINN loss {initial_pinn:.4f} ‚Üí {final_pinn:.4f} ({improvement:+.1f}% improvement)\")\n",
    "\n",
    "print(f\"\\nüìä WandB PINN Dashboard Features:\")\n",
    "print(f\"   ‚Ä¢ Physics loss component tracking (PINN, pressure gradient, wall shear stress)\")\n",
    "print(f\"   ‚Ä¢ Real-time physics violation monitoring\")\n",
    "print(f\"   ‚Ä¢ Pressure-velocity relationship analysis\")\n",
    "print(f\"   ‚Ä¢ Boundary layer physics adherence\")\n",
    "print(f\"   ‚Ä¢ Comparative physics consistency across experiments\")\n",
    "print(f\"   ‚Ä¢ Uncertainty quantification in physics-informed predictions\")\n",
    "\n",
    "# Find best performing approach\n",
    "best_idx = np.argmin(best_val_errors)\n",
    "best_name, _, _, _, _ = pinn_experiments[best_idx]\n",
    "\n",
    "print(f\"\\nüèÜ Best Overall Performance: {best_name} with Rel L2 = {min(best_val_errors):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ PINN-Enhanced Training Complete!\")\n",
    "print(f\"üß† Tested {len(pinn_experiments)} PINN configurations with physics constraints\")\n",
    "print(f\"üèÜ Best approach: {best_name}\")\n",
    "print(f\"üìä All physics losses and constraints logged to WandB\")\n",
    "print(f\"‚ö° Physics-informed learning successfully integrated\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PINN Integration Guide\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìö PINN INTEGRATION GUIDE FOR CFD SURROGATE MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüß† What are PINN Losses?\")\n",
    "print(\"   ‚Ä¢ Physics-Informed Neural Networks integrate physical laws into training\")\n",
    "print(\"   ‚Ä¢ Enforce CFD physics: continuity, momentum, energy conservation\")\n",
    "print(\"   ‚Ä¢ Improve generalization beyond pure data-driven approaches\")\n",
    "print(\"   ‚Ä¢ Ensure physically consistent predictions\")\n",
    "\n",
    "print(\"\\n‚ö° PINN Loss Components Implemented:\")\n",
    "print(\"   ‚Ä¢ compute_pinn_loss(): General physics-informed constraints\")\n",
    "print(\"   ‚Ä¢ compute_pressure_gradient_loss(): Pressure field consistency\")\n",
    "print(\"   ‚Ä¢ compute_wall_shear_stress_loss(): Boundary layer physics\")\n",
    "print(\"   ‚Ä¢ Navier-Stokes equation adherence\")\n",
    "print(\"   ‚Ä¢ Flow separation/attachment physics\")\n",
    "\n",
    "print(\"\\nüéØ Key Physics Relationships Enforced:\")\n",
    "print(\"   ‚Ä¢ Adverse pressure gradient ‚Üí low wall shear stress\")\n",
    "print(\"   ‚Ä¢ Favorable pressure gradient ‚Üí high wall shear stress\")\n",
    "print(\"   ‚Ä¢ Continuity equation satisfaction\")\n",
    "print(\"   ‚Ä¢ Momentum conservation\")\n",
    "print(\"   ‚Ä¢ No-slip boundary conditions\")\n",
    "\n",
    "print(\"\\nüîß PINN Loss Weight Tuning:\")\n",
    "print(\"   ‚Ä¢ Start with: pinn=0.1, pressure_gradient=0.1, wall_shear_stress=0.05\")\n",
    "print(\"   ‚Ä¢ Increase for stronger physics: pinn=0.3, pressure_gradient=0.2\")\n",
    "print(\"   ‚Ä¢ Balance with MSE loss: maintain data fidelity\")\n",
    "print(\"   ‚Ä¢ Monitor physics violation metrics in WandB\")\n",
    "\n",
    "print(\"\\nüìä Benefits of PINN Integration:\")\n",
    "print(\"   ‚Ä¢ Better extrapolation to unseen flow conditions\")\n",
    "print(\"   ‚Ä¢ More robust predictions in complex flow regions\")\n",
    "print(\"   ‚Ä¢ Reduced need for extensive training data\")\n",
    "print(\"   ‚Ä¢ Physically meaningful model behavior\")\n",
    "print(\"   ‚Ä¢ Enhanced model interpretability\")\n",
    "\n",
    "print(\"\\nüí° When to Use Different PINN Configurations:\")\n",
    "print(\"   ‚Ä¢ High PINN weights: Limited training data, need extrapolation\")\n",
    "print(\"   ‚Ä¢ Balanced weights: Good data coverage, need accuracy + physics\")\n",
    "print(\"   ‚Ä¢ Low PINN weights: Abundant data, prioritize fitting accuracy\")\n",
    "print(\"   ‚Ä¢ Ensemble + PINN: Maximum robustness + uncertainty quantification\")\n",
    "\n",
    "print(\"\\nüöÄ Advanced PINN Features:\")\n",
    "print(\"   ‚Ä¢ Automatic physics loss balancing\")\n",
    "print(\"   ‚Ä¢ Gradient-based physics constraints\")\n",
    "print(\"   ‚Ä¢ Boundary condition enforcement\")\n",
    "print(\"   ‚Ä¢ Multi-scale physics integration\")\n",
    "print(\"   ‚Ä¢ Adaptive physics weight scheduling\")\n",
    "\n",
    "print(\"\\nüîó Next Steps for PINN Development:\")\n",
    "print(\"   ‚Ä¢ Experiment with different physics weight combinations\")\n",
    "print(\"   ‚Ä¢ Monitor WandB physics consistency metrics\")\n",
    "print(\"   ‚Ä¢ Compare PINN vs standard training on test cases\")\n",
    "print(\"   ‚Ä¢ Validate physics adherence on complex geometries\")\n",
    "print(\"   ‚Ä¢ Explore domain-specific physics constraints\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß† Physics-Informed Neural Networks Successfully Integrated!\")\n",
    "print(\"üìà Enhanced CFD surrogate models with embedded physical laws\")\n",
    "print(\"üî¨ Ready for physically consistent flow field predictions\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg5090",
   "language": "python",
   "name": "pyg5090"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
