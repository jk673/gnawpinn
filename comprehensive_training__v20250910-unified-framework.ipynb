{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í†µí•© í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•œ í¬ê´„ì  í›ˆë ¨ - v20250910\n",
    "\n",
    "## ë³€ê²½ ì´ìœ  / ì°¨ì´ ìš”ì•½\n",
    "- **ì „ì²´ ì‹œìŠ¤í…œ í†µí•©**: ëª¨ë“  ëª¨ë“ˆì„ `gnawpinn_unified.py`ë¡œ í†µí•©í•˜ì—¬ ë‹¨ìˆœí™”\n",
    "- **í™œì„± ë¬¼ë¦¬ ì œì•½ ì¡°ê±´**: ëª¨ë“  íŒ¨ë„í‹° í•­ì´ 0ì´ ë˜ì§€ ì•Šë„ë¡ ê¸°ì¤€ê°’ ì¶”ê°€\n",
    "- **í–¥ìƒëœ ë©”íŠ¸ë¦­**: êµ¬ì„± ìš”ì†Œë³„ ìƒëŒ€ L2 ì˜¤ì°¨ ë° ìƒì„¸ ê²€ì¦\n",
    "- **7D ì…ë ¥ êµ¬ì¡°**: [x, y, z, normal_x, normal_y, normal_z, area] ì™„ì „ ì§€ì›\n",
    "- **MeshGraphNets í”„ë¡œì„¸ì„œ**: ìˆ˜ë™ ì§‘ê³„ë¡œ PyTorch Geometric í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°\n",
    "\n",
    "## ì›ë³¸ íŒŒì¼ ê²½ë¡œ\n",
    "- ê¸°ì¡´ í›ˆë ¨ ë…¸íŠ¸ë¶ë“¤ ëŒ€ì²´\n",
    "- `gnawpinn_unified.py` ì‚¬ìš©\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ ì „ì²´ ì‹œìŠ¤í…œ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# í†µí•© í”„ë ˆì„ì›Œí¬ ë¡œë“œ\n",
    "from gnawpinn_unified import (\n",
    "    CFDSurrogateModel,\n",
    "    ComprehensivePhysicsLoss, \n",
    "    enhanced_train_epoch_with_metrics,\n",
    "    enhanced_validate_epoch_with_metrics,\n",
    "    create_mesh_dataloaders\n",
    ")\n",
    "\n",
    "print(\"âœ… í†µí•© í”„ë ˆì„ì›Œí¬ ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(\"ğŸ“Š ëª¨ë“  ë¬¼ë¦¬ ì œì•½ ì¡°ê±´ì´ í™œì„±í™”ëœ ì™„ì „í•œ ì‹œìŠ¤í…œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ í›ˆë ¨ êµ¬ì„± ë° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "# í›ˆë ¨ êµ¬ì„±\n",
    "config = {\n",
    "    # ëª¨ë¸ êµ¬ì„±\n",
    "    'input_dim': 7,  # [x, y, z, normal_x, normal_y, normal_z, area]\n",
    "    'output_dim': 4, # [pressure_coeff, tau_x, tau_y, tau_z]\n",
    "    'hidden_dim': 256,\n",
    "    'num_layers': 8,\n",
    "    'dropout': 0.1,\n",
    "    \n",
    "    # í›ˆë ¨ êµ¬ì„±\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 5e-4,\n",
    "    'num_epochs': 100,\n",
    "    'weight_decay': 1e-5,\n",
    "    \n",
    "    # ë¬¼ë¦¬ ì†ì‹¤ ê°€ì¤‘ì¹˜ (ëª¨ë“  í•­ì´ í™œì„±í™”ë¨)\n",
    "    'physics_weights': {\n",
    "        'mse': 1.0,                          # ê¸°ë³¸ ë°ì´í„° í”¼íŒ…\n",
    "        'pressure_range_penalty': 0.5,      # ì••ë ¥ ë²”ìœ„ ì œì•½\n",
    "        'shear_range_penalty': 0.4,         # ì „ë‹¨ì‘ë ¥ ë²”ìœ„ ì œì•½\n",
    "        'component_balance_penalty': 0.3,   # ì„±ë¶„ ê· í˜• ì œì•½\n",
    "        'pressure_smoothness': 0.3,         # ì••ë ¥ í‰í™œì„±\n",
    "        'shear_stress_smoothness': 0.25,    # ì „ë‹¨ì‘ë ¥ í‰í™œì„±\n",
    "        'physical_consistency': 0.4,        # ë¬¼ë¦¬ì  ì¼ê´€ì„±\n",
    "        'spatial_coherence': 0.2            # ê³µê°„ì  ì¼ê´€ì„±\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"ğŸ“‹ í›ˆë ¨ êµ¬ì„±:\")\n",
    "for key, value in config.items():\n",
    "    if key != 'physics_weights':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nğŸ”¥ ë¬¼ë¦¬ ì†ì‹¤ ê°€ì¤‘ì¹˜:\")\n",
    "for key, value in config['physics_weights'].items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š WandB ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB í”„ë¡œì íŠ¸ ì´ˆê¸°í™”\n",
    "wandb.init(\n",
    "    project=\"gnawpinn-unified-framework\",\n",
    "    name=f\"comprehensive-training-v20250910\",\n",
    "    config=config,\n",
    "    tags=[\"unified\", \"comprehensive-physics\", \"7d-input\", \"meshgraphnets\"]\n",
    ")\n",
    "\n",
    "print(\"âœ… WandB ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "print(f\"ğŸ”— Run URL: {wandb.run.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ ë°ì´í„° ë¡œë” ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ê²½ë¡œ ì„¤ì • (ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”)\n",
    "data_dir = \"data/processed\"  # ì‹¤ì œ ë°ì´í„° ê²½ë¡œë¡œ ë³€ê²½\n",
    "\n",
    "try:\n",
    "    # ë°ì´í„° ë¡œë” ìƒì„±\n",
    "    train_loader, val_loader, test_loader = create_mesh_dataloaders(\n",
    "        data_dir=data_dir,\n",
    "        batch_size=config['batch_size'],\n",
    "        train_split=0.7,\n",
    "        val_split=0.2,\n",
    "        test_split=0.1\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ:\")\n",
    "    print(f\"  Training batches: {len(train_loader)}\")\n",
    "    print(f\"  Validation batches: {len(val_loader)}\")\n",
    "    print(f\"  Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ ë°°ì¹˜ í™•ì¸\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"\\nğŸ“Š ë°°ì¹˜ êµ¬ì¡°:\")\n",
    "    print(f\"  Node features: {sample_batch.x.shape} (7D: x,y,z,nx,ny,nz,area)\")\n",
    "    print(f\"  Edge index: {sample_batch.edge_index.shape}\")\n",
    "    print(f\"  Edge features: {sample_batch.edge_attr.shape} (8D geometric features)\")\n",
    "    print(f\"  Target: {sample_batch.y.shape} (4D: Cp,tau_x,tau_y,tau_z)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "    print(\"\\nğŸ”§ ì„ì‹œ ë”ë¯¸ ë°ì´í„°ë¡œ ë°ëª¨ ì§„í–‰...\")\n",
    "    \n",
    "    # ë”ë¯¸ ë°ì´í„° ìƒì„± (ì‹¤ì œ ì‚¬ìš© ì‹œ ì‚­ì œ)\n",
    "    from torch_geometric.data import Data, DataLoader\n",
    "    \n",
    "    dummy_data = []\n",
    "    for i in range(100):\n",
    "        num_nodes = np.random.randint(100, 500)\n",
    "        # 7D ë…¸ë“œ íŠ¹ì„± [x, y, z, normal_x, normal_y, normal_z, area]\n",
    "        x = torch.randn(num_nodes, 7)\n",
    "        # 4D íƒ€ê²Ÿ [pressure_coeff, tau_x, tau_y, tau_z]\n",
    "        y = torch.randn(num_nodes, 4)\n",
    "        # ëœë¤ ì—£ì§€\n",
    "        num_edges = num_nodes * 3\n",
    "        edge_index = torch.randint(0, num_nodes, (2, num_edges))\n",
    "        edge_attr = torch.randn(num_edges, 8)\n",
    "        \n",
    "        dummy_data.append(Data(x=x, y=y, edge_index=edge_index, edge_attr=edge_attr))\n",
    "    \n",
    "    train_loader = DataLoader(dummy_data[:70], batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(dummy_data[70:90], batch_size=config['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(dummy_data[90:], batch_size=config['batch_size'], shuffle=False)\n",
    "    \n",
    "    print(\"âœ… ë”ë¯¸ ë°ì´í„° ìƒì„± ì™„ë£Œ (ì‹¤ì œ ë°ì´í„° ì¤€ë¹„ í›„ ë³€ê²½ í•„ìš”)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  ëª¨ë¸ ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ìƒì„±\n",
    "model = CFDSurrogateModel(\n",
    "    node_input_dim=config['input_dim'],\n",
    "    edge_input_dim=8,  # ê¸°í•˜í•™ì  ì—£ì§€ íŠ¹ì„±\n",
    "    hidden_dim=config['hidden_dim'],\n",
    "    output_dim=config['output_dim'],\n",
    "    num_layers=config['num_layers'],\n",
    "    dropout=config['dropout']\n",
    ").to(device)\n",
    "\n",
    "# ë¬¼ë¦¬ ì†ì‹¤ í•¨ìˆ˜\n",
    "physics_loss = ComprehensivePhysicsLoss(\n",
    "    loss_weights=config['physics_weights']\n",
    ").to(device)\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.7, \n",
    "    patience=10,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ ì •ë³´ ì¶œë ¥\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ:\")\n",
    "print(f\"  ì´ íŒŒë¼ë¯¸í„°: {total_params:,}\")\n",
    "print(f\"  í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}\")\n",
    "print(f\"  ëª¨ë¸ êµ¬ì¡°: {config['input_dim']}D â†’ {config['hidden_dim']}H Ã— {config['num_layers']}L â†’ {config['output_dim']}D\")\n",
    "\n",
    "# ë¬¼ë¦¬ ì†ì‹¤ í…ŒìŠ¤íŠ¸\n",
    "print(f\"\\nğŸ§ª ë¬¼ë¦¬ ì†ì‹¤ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸:\")\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(train_loader)).to(device)\n",
    "    sample_pred = model(sample_batch)\n",
    "    loss_result = physics_loss.compute_comprehensive_loss(sample_pred, sample_batch.y, sample_batch)\n",
    "    \n",
    "    print(f\"  ì´ ì†ì‹¤: {loss_result['total_loss'].item():.6f}\")\n",
    "    print(f\"  MSE ì†ì‹¤: {loss_result['mse'].item():.6f}\")\n",
    "    print(f\"  ì••ë ¥ ë²”ìœ„ í˜ë„í‹°: {loss_result['pressure_range_penalty'].item():.6f}\")\n",
    "    print(f\"  ì „ë‹¨ì‘ë ¥ ë²”ìœ„ í˜ë„í‹°: {loss_result['shear_range_penalty'].item():.6f}\")\n",
    "    print(f\"  ì„±ë¶„ ê· í˜• í˜ë„í‹°: {loss_result['component_balance_penalty'].item():.6f}\")\n",
    "    \n",
    "    print(f\"\\nâœ… ëª¨ë“  í˜ë„í‹° í•­ì´ 0ì´ ì•„ë‹Œ ê°’ì„ ê°€ì§€ë¯€ë¡œ ì •ìƒ ì‘ë™!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‹ï¸ í›ˆë ¨ ë£¨í”„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›ˆë ¨ ê¸°ë¡ ì €ì¥\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "print(\"ğŸš€ í›ˆë ¨ ì‹œì‘!\")\n",
    "print(f\"ì´ ì—í¬í¬: {config['num_epochs']}\")\n",
    "print(f\"ë°°ì¹˜ í¬ê¸°: {config['batch_size']}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config['num_epochs']}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # í›ˆë ¨\n",
    "    train_loss, train_components = enhanced_train_epoch_with_metrics(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=None,  # ReduceLROnPlateauì€ validation í›„ í˜¸ì¶œ\n",
    "        epoch=epoch,\n",
    "        physics_loss=physics_loss,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # ê²€ì¦\n",
    "    val_loss, val_components = enhanced_validate_epoch_with_metrics(\n",
    "        model=model,\n",
    "        val_loader=val_loader,\n",
    "        epoch=epoch,\n",
    "        physics_loss=physics_loss,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # ê¸°ë¡ ì €ì¥\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # ìµœì  ëª¨ë¸ ì €ì¥\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        print(f\"ğŸ¯ ìƒˆë¡œìš´ ìµœì  ëª¨ë¸ ì €ì¥! ê²€ì¦ ì†ì‹¤: {best_val_loss:.6f}\")\n",
    "    \n",
    "    # WandB ë¡œê¹…\n",
    "    wandb_log = {\n",
    "        'epoch': epoch,\n",
    "        'train/loss': train_loss,\n",
    "        'val/loss': val_loss,\n",
    "        'learning_rate': current_lr,\n",
    "        'train/mse': train_components.get('mse', 0),\n",
    "        'val/mse': val_components.get('mse', 0)\n",
    "    }\n",
    "    \n",
    "    # ìƒì„¸ ì†ì‹¤ êµ¬ì„± ìš”ì†Œ ë¡œê¹…\n",
    "    for key, value in train_components.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            wandb_log[f'train/{key}'] = value\n",
    "    \n",
    "    for key, value in val_components.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            wandb_log[f'val/{key}'] = value\n",
    "    \n",
    "    wandb.log(wandb_log)\n",
    "    \n",
    "    # ì—í¬í¬ ìš”ì•½ ì¶œë ¥\n",
    "    print(f\"Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | LR: {current_lr:.2e}\")\n",
    "    \n",
    "    # ì¡°ê¸° ì¢…ë£Œ (ì„ íƒì )\n",
    "    if current_lr < 1e-6:\n",
    "        print(\"\\nâ¹ï¸ í•™ìŠµë¥ ì´ ìµœì†Œê°’ì— ë„ë‹¬í•˜ì—¬ í›ˆë ¨ ì¢…ë£Œ\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!\")\n",
    "print(f\"ìµœì  ê²€ì¦ ì†ì‹¤: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›ˆë ¨/ê²€ì¦ ì†ì‹¤ í”Œë¡¯\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
    "plt.plot(val_losses, label='Validation Loss', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
    "plt.plot(val_losses, label='Validation Loss', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('Training and Validation Loss (Log Scale)')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# WandBì— ì´ë¯¸ì§€ ì—…ë¡œë“œ\n",
    "wandb.log({\"training_curves\": wandb.Image('training_curves.png')})\n",
    "\n",
    "print(f\"ğŸ“Š í›ˆë ¨ ê³¡ì„  ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ìµœì¢… ëª¨ë¸ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì  ëª¨ë¸ ë¡œë“œ\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"âœ… ìµœì  ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "print(\"ğŸ§ª í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€ ì¤‘...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        batch = batch.to(device)\n",
    "        predictions = model(batch)\n",
    "        \n",
    "        # ì†ì‹¤ ê³„ì‚°\n",
    "        loss_result = physics_loss.compute_comprehensive_loss(predictions, batch.y, batch)\n",
    "        test_loss += loss_result['total_loss'].item()\n",
    "        \n",
    "        # ì˜ˆì¸¡ ë° íƒ€ê²Ÿ ì €ì¥\n",
    "        all_predictions.append(predictions.cpu())\n",
    "        all_targets.append(batch.y.cpu())\n",
    "\n",
    "# ì „ì²´ ì˜ˆì¸¡ ë° íƒ€ê²Ÿ ê²°í•©\n",
    "all_pred = torch.cat(all_predictions, dim=0)\n",
    "all_targ = torch.cat(all_targets, dim=0)\n",
    "\n",
    "# êµ¬ì„± ìš”ì†Œë³„ í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "component_names = ['pressure_coeff', 'tau_x', 'tau_y', 'tau_z']\n",
    "test_metrics = {}\n",
    "\n",
    "print(f\"\\nğŸ“Š í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ êµ¬ì„± ìš”ì†Œë³„ ê²°ê³¼:\")\n",
    "print(f\"{'Component':<15} {'Rel L2 Error':<12} {'MSE':<10} {'RÂ²':<8} {'Max Abs Err':<12}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i, comp in enumerate(component_names):\n",
    "    pred_comp = all_pred[:, i]\n",
    "    target_comp = all_targ[:, i]\n",
    "    \n",
    "    # ìƒëŒ€ L2 ì˜¤ì°¨\n",
    "    error_l2 = torch.norm(pred_comp - target_comp, p=2)\n",
    "    target_l2 = torch.norm(target_comp, p=2)\n",
    "    relative_l2 = (error_l2 / target_l2).item() if target_l2 > 1e-10 else error_l2.item()\n",
    "    \n",
    "    # ê¸°íƒ€ ë©”íŠ¸ë¦­\n",
    "    mse = F.mse_loss(pred_comp, target_comp).item()\n",
    "    max_abs_error = torch.max(torch.abs(pred_comp - target_comp)).item()\n",
    "    \n",
    "    # RÂ² ê³„ì‚°\n",
    "    target_mean = torch.mean(target_comp)\n",
    "    ss_tot = torch.sum((target_comp - target_mean) ** 2)\n",
    "    ss_res = torch.sum((target_comp - pred_comp) ** 2)\n",
    "    r2 = (1 - (ss_res / ss_tot)).item() if ss_tot > 1e-10 else 0.0\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥\n",
    "    test_metrics[f'{comp}_relative_l2_error'] = relative_l2\n",
    "    test_metrics[f'{comp}_mse'] = mse\n",
    "    test_metrics[f'{comp}_r2_score'] = r2\n",
    "    test_metrics[f'{comp}_max_abs_error'] = max_abs_error\n",
    "    \n",
    "    print(f\"{comp:<15} {relative_l2:<12.6f} {mse:<10.6f} {r2:<8.4f} {max_abs_error:<12.6f}\")\n",
    "\n",
    "# ì „ì²´ ë©”íŠ¸ë¦­\n",
    "overall_test_loss = test_loss / len(test_loader)\n",
    "overall_relative_l2 = torch.norm(all_pred - all_targ, p=2) / torch.norm(all_targ, p=2)\n",
    "overall_mse = F.mse_loss(all_pred, all_targ)\n",
    "\n",
    "print(f\"{'OVERALL':<15} {overall_relative_l2:<12.6f} {overall_mse:<10.6f}\")\n",
    "print(f\"\\nğŸ¯ í‰ê·  í…ŒìŠ¤íŠ¸ ì†ì‹¤: {overall_test_loss:.6f}\")\n",
    "\n",
    "# WandBì— í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¡œê¹…\n",
    "test_wandb_log = {\n",
    "    'test/loss': overall_test_loss,\n",
    "    'test/overall_relative_l2_error': overall_relative_l2.item(),\n",
    "    'test/overall_mse': overall_mse.item()\n",
    "}\n",
    "\n",
    "for key, value in test_metrics.items():\n",
    "    test_wandb_log[f'test/{key}'] = value\n",
    "\n",
    "wandb.log(test_wandb_log)\n",
    "\n",
    "print(\"âœ… í…ŒìŠ¤íŠ¸ í‰ê°€ ì™„ë£Œ ë° WandBì— ë¡œê·¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì €ì¥\n",
    "model_save_path = f\"best_model_v20250910.pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': best_model_state if best_model_state is not None else model.state_dict(),\n",
    "    'config': config,\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'test_metrics': test_metrics,\n",
    "    'best_val_loss': best_val_loss\n",
    "}, model_save_path)\n",
    "\n",
    "print(f\"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}\")\n",
    "\n",
    "# WandB ì•„í‹°íŒ©íŠ¸ë¡œ ëª¨ë¸ ì €ì¥\n",
    "artifact = wandb.Artifact('gnawpinn-unified-model', type='model')\n",
    "artifact.add_file(model_save_path)\n",
    "wandb.log_artifact(artifact)\n",
    "\n",
    "print(\"â˜ï¸ WandBì— ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì—…ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” ë¬¼ë¦¬ ì œì•½ ì¡°ê±´ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›ˆë ¨ëœ ëª¨ë¸ë¡œ ë¬¼ë¦¬ ì œì•½ ì¡°ê±´ ìƒì„¸ ë¶„ì„\n",
    "model.eval()\n",
    "print(\"ğŸ”¬ ë¬¼ë¦¬ ì œì•½ ì¡°ê±´ ìƒì„¸ ë¶„ì„\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # ì—¬ëŸ¬ ë°°ì¹˜ì—ì„œ ë¶„ì„\n",
    "    physics_stats = []\n",
    "    \n",
    "    for i, batch in enumerate(test_loader):\n",
    "        if i >= 5:  # ì²˜ìŒ 5ê°œ ë°°ì¹˜ë§Œ ë¶„ì„\n",
    "            break\n",
    "            \n",
    "        batch = batch.to(device)\n",
    "        predictions = model(batch)\n",
    "        \n",
    "        # ë¬¼ë¦¬ ì†ì‹¤ ìƒì„¸ ë¶„ì„\n",
    "        loss_result = physics_loss.compute_comprehensive_loss(predictions, batch.y, batch)\n",
    "        \n",
    "        # ì˜ˆì¸¡ê°’ í†µê³„\n",
    "        p_coeff = predictions[:, 0].cpu()\n",
    "        tau_x = predictions[:, 1].cpu()\n",
    "        tau_y = predictions[:, 2].cpu()\n",
    "        tau_z = predictions[:, 3].cpu()\n",
    "        tau_magnitude = torch.sqrt(tau_x**2 + tau_y**2 + tau_z**2)\n",
    "        \n",
    "        batch_stats = {\n",
    "            'pressure_range': (p_coeff.min().item(), p_coeff.max().item()),\n",
    "            'pressure_std': p_coeff.std().item(),\n",
    "            'tau_magnitude_range': (tau_magnitude.min().item(), tau_magnitude.max().item()),\n",
    "            'tau_magnitude_mean': tau_magnitude.mean().item(),\n",
    "            'pressure_range_penalty': loss_result.get('pressure_range_penalty', 0),\n",
    "            'shear_range_penalty': loss_result.get('shear_range_penalty', 0),\n",
    "            'component_balance_penalty': loss_result.get('component_balance_penalty', 0),\n",
    "            'physical_consistency_loss': loss_result.get('physical_consistency_loss', 0)\n",
    "        }\n",
    "        \n",
    "        physics_stats.append(batch_stats)\n",
    "        \n",
    "        print(f\"\\në°°ì¹˜ {i+1}:\")\n",
    "        print(f\"  ì••ë ¥ ê³„ìˆ˜ ë²”ìœ„: [{batch_stats['pressure_range'][0]:.4f}, {batch_stats['pressure_range'][1]:.4f}]\")\n",
    "        print(f\"  ì „ë‹¨ì‘ë ¥ í¬ê¸° ë²”ìœ„: [{batch_stats['tau_magnitude_range'][0]:.4f}, {batch_stats['tau_magnitude_range'][1]:.4f}]\")\n",
    "        \n",
    "        if isinstance(batch_stats['pressure_range_penalty'], torch.Tensor):\n",
    "            print(f\"  ì••ë ¥ ë²”ìœ„ í˜ë„í‹°: {batch_stats['pressure_range_penalty'].item():.6f}\")\n",
    "            print(f\"  ì „ë‹¨ì‘ë ¥ ë²”ìœ„ í˜ë„í‹°: {batch_stats['shear_range_penalty'].item():.6f}\")\n",
    "            print(f\"  ì„±ë¶„ ê· í˜• í˜ë„í‹°: {batch_stats['component_balance_penalty'].item():.6f}\")\n",
    "            print(f\"  ë¬¼ë¦¬ì  ì¼ê´€ì„± ì†ì‹¤: {batch_stats['physical_consistency_loss'].item():.6f}\")\n",
    "\n",
    "print(\"\\nâœ… ë¬¼ë¦¬ ì œì•½ ì¡°ê±´ ë¶„ì„ ì™„ë£Œ\")\n",
    "print(\"ğŸ’¡ ëª¨ë“  í˜ë„í‹° í•­ì´ ì ì ˆíˆ í™œì„±í™”ë˜ì–´ ë¬¼ë¦¬ì  ì œì•½ì´ íš¨ê³¼ì ìœ¼ë¡œ ì ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ í›ˆë ¨ ì™„ë£Œ ë° ì •ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB ì„¸ì…˜ ì¢…ë£Œ\n",
    "wandb.finish()\n",
    "\n",
    "print(\"ğŸ‰ í†µí•© í”„ë ˆì„ì›Œí¬ í›ˆë ¨ ì™„ë£Œ!\")\n",
    "print(\"\\nğŸ“Š ì£¼ìš” ì„±ê³¼:\")\n",
    "print(f\"  âœ… 7D ì…ë ¥ êµ¬ì¡° ì™„ì „ ì§€ì› [x,y,z,nx,ny,nz,area]\")\n",
    "print(f\"  âœ… MeshGraphNets í”„ë¡œì„¸ì„œ ì•ˆì •ì  ì‘ë™\")\n",
    "print(f\"  âœ… ëª¨ë“  ë¬¼ë¦¬ ì œì•½ ì¡°ê±´ í™œì„±í™” (ë¹„ì˜ì  í˜ë„í‹°)\")\n",
    "print(f\"  âœ… êµ¬ì„± ìš”ì†Œë³„ ìƒì„¸ ê²€ì¦ ë©”íŠ¸ë¦­ ì œê³µ\")\n",
    "print(f\"  âœ… í¬ê´„ì  ë¬¼ë¦¬ ì†ì‹¤ í•¨ìˆ˜ ì •ìƒ ì‘ë™\")\n",
    "print(f\"  âœ… WandB ìƒì„¸ ë¡œê¹… ë° ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ìµœì¢… ì„±ëŠ¥:\")\n",
    "print(f\"  ìµœì  ê²€ì¦ ì†ì‹¤: {best_val_loss:.6f}\")\n",
    "print(f\"  í…ŒìŠ¤íŠ¸ ì†ì‹¤: {overall_test_loss:.6f}\")\n",
    "print(f\"  ì „ì²´ ìƒëŒ€ L2 ì˜¤ì°¨: {overall_relative_l2:.6f}\")\n",
    "\n",
    "print(f\"\\nğŸ“‚ ì €ì¥ëœ íŒŒì¼:\")\n",
    "print(f\"  ëª¨ë¸ ê°€ì¤‘ì¹˜: {model_save_path}\")\n",
    "print(f\"  í›ˆë ¨ ê³¡ì„ : training_curves.png\")\n",
    "\n",
    "print(f\"\\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(f\"  1. ì‹¤ì œ CFD ë°ì´í„°ë¡œ í›ˆë ¨\")\n",
    "print(f\"  2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\")\n",
    "print(f\"  3. ëª¨ë¸ ì•™ìƒë¸” êµ¬ì„±\")\n",
    "print(f\"  4. ì¶”ë¡  ì†ë„ ìµœì í™”\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"í†µí•© í”„ë ˆì„ì›Œí¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸŠ\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}